#!/usr/bin/env python3
"""
LNN-AGI: XL Agent

2B íŒŒë¼ë¯¸í„° ëŒ€ê·œëª¨ ëª¨ë¸ + íŠ¸ë ˆì´ë”©/ìë™í™” ê¸°ëŠ¥.
ëª©í‘œ: ëˆ ë²„ëŠ” AI
"""

import asyncio
import time
import json
import numpy as np
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, List, Callable

import torch
import torch.nn.functional as F
from PIL import Image

from .plastic_lnn_xl import PlasticVisionLNNXL, LNNXLConfig, create_lnn_xl


@dataclass
class AgentXLConfig:
    """XL Agent ì„¤ì •."""
    # í™”ë©´
    screen_width: int = 1280
    screen_height: int = 720

    # ëª¨ë¸
    model_size: str = 'large'  # 'base', 'large', 'xl'

    # í•™ìŠµ
    online_learning: bool = True
    exploration_rate: float = 0.2
    exploration_decay: float = 0.9995
    min_exploration: float = 0.05

    # ì•¡ì…˜
    action_interval: float = 0.3  # ì´ˆ

    # ì²´í¬í¬ì¸íŠ¸
    checkpoint_dir: str = "checkpoints_xl"
    checkpoint_interval: int = 100

    # íŠ¸ë ˆì´ë”©
    trading_enabled: bool = True
    trading_log_file: str = "trading_signals.jsonl"

    # íƒœìŠ¤í¬
    active_tasks: List[str] = field(default_factory=lambda: [
        'action', 'trading', 'analysis', 'planning'
    ])


@dataclass
class AgentXLState:
    """Agent ìƒíƒœ."""
    step: int = 0
    total_reward: float = 0.0
    exploration_rate: float = 0.2
    session_start: float = field(default_factory=time.time)

    # í†µê³„
    actions_taken: int = 0
    clicks_made: int = 0
    trading_signals: int = 0
    profitable_signals: int = 0


class LivingAgentXL:
    """
    2B íŒŒë¼ë¯¸í„° Living Agent.

    íŠ¹ì§•:
    - ëŒ€ê·œëª¨ Plastic LNN (2B params, 16GB VRAM)
    - ì‹¤ì‹œê°„ ì˜¨ë¼ì¸ í•™ìŠµ
    - íŠ¸ë ˆì´ë”© ì‹ í˜¸ ìƒì„±
    - í™”ë©´ ë¶„ì„ ë° ìë™í™”
    """

    def __init__(
        self,
        config: Optional[AgentXLConfig] = None,
        device: str = 'cuda',
    ):
        self.config = config or AgentXLConfig()
        self.device = device
        self.state = AgentXLState(exploration_rate=self.config.exploration_rate)

        # í™˜ê²½
        self._env = None

        # ëª¨ë¸
        print("=" * 60)
        print("  LNN-AGI: Initializing 2B Parameter Agent")
        print("=" * 60)
        self._model = create_lnn_xl(device, self.config.model_size)
        self._model.train()

        # ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬
        Path(self.config.checkpoint_dir).mkdir(exist_ok=True)

        # íŠ¸ë ˆì´ë”© ë¡œê·¸
        if self.config.trading_enabled:
            self._trading_log = open(self.config.trading_log_file, 'a')

        # ì½œë°±
        self.on_action: Optional[Callable] = None
        self.on_reward: Optional[Callable] = None
        self.on_trading_signal: Optional[Callable] = None

        # ì´ì „ ì¶œë ¥ ì €ì¥
        self._prev_output: Optional[Dict] = None
        self._prev_screen: Optional[np.ndarray] = None

        print(f"âœ“ Agent initialized on {device}")

    async def connect_environment(self, env):
        """í™˜ê²½ ì—°ê²°."""
        self._env = env
        print(f"âœ“ Connected to environment: ({env.width}, {env.height})")

    def _preprocess_screen(self, screen: np.ndarray) -> torch.Tensor:
        """í™”ë©´ ì „ì²˜ë¦¬."""
        # Resize if needed
        img = Image.fromarray(screen)
        if img.size != (self.config.screen_width, self.config.screen_height):
            img = img.resize((self.config.screen_width, self.config.screen_height))

        # To tensor
        arr = np.array(img).astype(np.float32) / 255.0
        tensor = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)

        return tensor.to(self.device)

    def _compute_reward(
        self,
        prev_screen: np.ndarray,
        curr_screen: np.ndarray,
        action: Dict[str, Any],
    ) -> float:
        """ë³´ìƒ ê³„ì‚°."""
        reward = 0.0

        # 1. í™”ë©´ ë³€í™” ë³´ìƒ
        prev_gray = np.mean(prev_screen, axis=2)
        curr_gray = np.mean(curr_screen, axis=2)
        diff = np.abs(curr_gray - prev_gray).mean() / 255.0

        if diff > 0.01:  # ì˜ë¯¸ìˆëŠ” ë³€í™”
            reward += diff * 2.0  # ë³€í™”ì— ë¹„ë¡€í•˜ëŠ” ë³´ìƒ

        # 2. í´ë¦­ ì—”íŠ¸ë¡œí”¼ (ë¶ˆí•„ìš”í•œ í´ë¦­ íŒ¨ë„í‹°)
        if action.get('click') != 'none':
            if diff < 0.01:  # í´ë¦­í–ˆëŠ”ë° ë³€í™” ì—†ìŒ
                reward -= 0.1
            else:  # í´ë¦­ìœ¼ë¡œ ë³€í™” ë°œìƒ
                reward += 0.2

        # 3. íƒí—˜ ë³´ë„ˆìŠ¤
        x, y = action.get('x', 0.5), action.get('y', 0.5)
        edge_bonus = 0.0
        if x < 0.1 or x > 0.9 or y < 0.1 or y > 0.9:
            edge_bonus = 0.02  # ê°€ì¥ìë¦¬ íƒí—˜ ë³´ë„ˆìŠ¤
        reward += edge_bonus

        return reward

    def _decode_action(self, output: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """ëª¨ë¸ ì¶œë ¥ì—ì„œ ì•¡ì…˜ ì¶”ì¶œ."""
        action_tensor = output['action'][0]  # [output_dim]

        # x, y ì¢Œí‘œ (0~1)
        x = torch.sigmoid(action_tensor[0]).item()
        y = torch.sigmoid(action_tensor[1]).item()

        # í´ë¦­ íƒ€ì…
        click_logits = action_tensor[2:5]
        click_idx = torch.softmax(click_logits, dim=0).argmax().item()
        click = ['none', 'left', 'right'][click_idx]

        # í‚¤ ì…ë ¥ (ì•„ì§ ë¯¸êµ¬í˜„)
        keys = []

        return {
            'x': x,
            'y': y,
            'click': click,
            'keys': keys,
        }

    def _log_trading_signal(self, signal: Dict[str, Any]):
        """íŠ¸ë ˆì´ë”© ì‹ í˜¸ ë¡œê¹…."""
        if not self.config.trading_enabled:
            return

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'step': self.state.step,
            **signal,
        }
        self._trading_log.write(json.dumps(log_entry) + '\n')
        self._trading_log.flush()

        self.state.trading_signals += 1

        if self.on_trading_signal:
            self.on_trading_signal(signal)

    async def step(self) -> Dict[str, Any]:
        """í•œ ìŠ¤í… ì‹¤í–‰."""
        assert self._env is not None, "Environment not connected"

        self.state.step += 1
        self.state.actions_taken += 1

        # 1. í™”ë©´ ìº¡ì²˜
        screen = self._env.capture_screen()
        screen_tensor = self._preprocess_screen(screen)

        # 2. ëª¨ë¸ ì¶”ë¡ 
        self._model.train()
        with torch.amp.autocast('cuda'):
            output = self._model(screen_tensor, self.config.active_tasks)

        # 3. íƒí—˜ vs í™œìš©
        if np.random.random() < self.state.exploration_rate:
            # ëœë¤ íƒí—˜
            action = {
                'x': np.random.random(),
                'y': np.random.random(),
                'click': np.random.choice(['none', 'left', 'right'], p=[0.7, 0.2, 0.1]),
                'keys': [],
            }
        else:
            # ëª¨ë¸ ì•¡ì…˜
            action = self._decode_action(output)

        # 4. ì•¡ì…˜ ì‹¤í–‰
        self._env.inject_action(action)
        if action['click'] != 'none':
            self.state.clicks_made += 1

        if self.on_action:
            self.on_action(action)

        # ëŒ€ê¸°
        await asyncio.sleep(self.config.action_interval)

        # 5. ê²°ê³¼ ê´€ì°°
        next_screen = self._env.capture_screen()
        next_tensor = self._preprocess_screen(next_screen)

        with torch.amp.autocast('cuda'):
            next_output = self._model(next_tensor, self.config.active_tasks)

        # 6. ë³´ìƒ ê³„ì‚°
        reward = self._compute_reward(screen, next_screen, action)
        self.state.total_reward += reward

        if self.on_reward and abs(reward) > 0.1:
            self.on_reward(reward)

        # 7. ì˜¨ë¼ì¸ í•™ìŠµ
        # TD-learning: V(s) + reward -> V(s')
        # output has gradients (current), next_output is target (detached)
        if self.config.online_learning:
            # Detach next_output for use as target
            next_out_detached = {k: v.detach() if isinstance(v, torch.Tensor) else v
                                 for k, v in next_output.items()}
            self._model.plastic_lnn.online_update(
                reward,
                output,  # Has gradients
                next_out_detached,  # Target (no grad)
            )

        # 8. íŠ¸ë ˆì´ë”© ì‹ í˜¸ ì²˜ë¦¬
        if 'trading' in output and self.config.trading_enabled:
            signal = self._model.plastic_lnn.get_trading_signal(output)
            if signal['confidence'] > 0.6:  # ì‹ ë¢°ë„ 60% ì´ìƒë§Œ
                self._log_trading_signal(signal)

        # 9. íƒí—˜ìœ¨ ê°ì†Œ
        self.state.exploration_rate = max(
            self.config.min_exploration,
            self.state.exploration_rate * self.config.exploration_decay
        )

        return {
            'step': self.state.step,
            'action': action,
            'reward': reward,
            'exploration_rate': self.state.exploration_rate,
        }

    async def run_forever(self):
        """ë¬´í•œ ì‹¤í–‰ ë£¨í”„."""
        print("\n" + "=" * 60)
        print("LNN-AGI: Starting 2B Parameter Agent")
        print("=" * 60 + "\n")

        while True:
            try:
                step_info = await self.step()

                # ì£¼ê¸°ì  ì¶œë ¥
                if self.state.step % 50 == 0:
                    elapsed = time.time() - self.state.session_start
                    steps_per_sec = self.state.step / elapsed if elapsed > 0 else 0

                    print(f"\n[Step {self.state.step}]")
                    print(f"  Total reward: {self.state.total_reward:.2f}")
                    print(f"  Exploration: {self.state.exploration_rate:.3f}")
                    print(f"  Speed: {steps_per_sec:.2f} steps/sec")
                    print(f"  Trading signals: {self.state.trading_signals}")

                # ì²´í¬í¬ì¸íŠ¸
                if self.state.step % self.config.checkpoint_interval == 0:
                    self._save_checkpoint()

            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"Error at step {self.state.step}: {e}")
                await asyncio.sleep(1)

        print("\nâœ“ Agent stopped")

    def _save_checkpoint(self):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥."""
        path = Path(self.config.checkpoint_dir) / f"lnn_agi_step_{self.state.step}.pt"
        self._model.plastic_lnn.save(str(path))
        print(f"âœ“ Checkpoint saved: {path}")

    def load_checkpoint(self, path: str):
        """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ."""
        from .plastic_lnn_xl import PlasticVisionLNNXL
        self._model = PlasticVisionLNNXL.load(path, self.device)
        print(f"âœ“ Checkpoint loaded: {path}")

    def set_goal(self, goal: str):
        """ëª©í‘œ ì„¤ì • (ì¶”í›„ goal-conditioned learning)."""
        print(f"ğŸ¯ Goal set: {goal}")
        # TODO: goal encoding and conditioning

    def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬."""
        if hasattr(self, '_trading_log'):
            self._trading_log.close()


def create_agent_xl(device: str = 'cuda') -> LivingAgentXL:
    """XL Agent ìƒì„±."""
    config = AgentXLConfig(
        model_size='large',
        online_learning=True,
        trading_enabled=True,
    )
    return LivingAgentXL(config, device)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    agent = create_agent_xl('cuda')
    print("\nâœ“ Agent created successfully")
